{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Driven Presales Evaluation\n",
    "\n",
    "Welcome to using the data driven jupyter notebook for evaluating if data from csv file(s) is sufficient for creating simulators using supervised learning methods. The approach of learning the state transitions, $(\\underline{s}, \\underline{a}) \\rightarrow \\underline{s}'$ from data is growing in popularity, however, not all of your data may have the correct distributions for the ranges needed for your Reinforcement Learning use case. This notebook is split up into three sections: \n",
    "\n",
    "- Data Relevance\n",
    "- Sparsity\n",
    "- Data Distribution Confidence\n",
    "\n",
    "This notebook uses `nbgrader` package to 'grade' your data quality, distributions, and feasibility of creating approximated simulations from your data. A score of 100 means you passed all the tests. The tests basically consists of assert conditions that individual notebook cells must run successfully, require user input of `Y/N` that you agree, or requiring inputs of Subject Matter Expert (SME) data ranges. The `nbgrader` package allows for certain snippets of code to be hidden from you to simplify the usage of this notebook. When code is hidden from you, you will know because the cell can NO longer be edited in jupyter notebook.\n",
    "\n",
    "> To pass tests, you may have to create a new cell and write code to filter/smooth/manipulate your data \n",
    "\n",
    "Successfully run all cells to assess whether or not a data driven simulator can be adequately created from your data. Once you have ran the cells without assertion errors, quickly double check your script passes the tests by click the `Validate` button in the jupyter notebook. If all tests are passed, then please export this as a PDF to share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "gitroot = os.popen('git rev-parse --show-toplevel').read()\n",
    "os.chdir(gitroot.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION A: Data Relevance\n",
    "\n",
    "- load csv file(s)\n",
    "- define potential inputs/outputs\n",
    "- check NaN\n",
    "- define cadence of state transitions\n",
    "- check outliers and plot each indiviual dataset\n",
    "- check NaN after concatenating\n",
    "- use feature importances to determine best features\n",
    "- re-define features (inputs)\n",
    "- save as single csv, named approved_data.csv\n",
    "\n",
    "`Step 1`: Add path to filenames as strings to the `filenames` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    'example_data.csv',\n",
    "    'data.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test below simply awards 10 points if the data can be successfuly loaded into the jupyter notebook, i.e. paths are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "396aa85d1b6acd8de71f4c0fc9a44cef",
     "grade": true,
     "grade_id": "cell-f646fd7fefe9fed1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for location in filenames:\n",
    "    read_datafile = pd.read_csv(location)\n",
    "    df_list.append(read_datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 2`: Add any potential feature names as states and actions to the dictionary in the following way. Here we want to be more broad than you think because this notebook will help you determine which features probably matter more using what's called feature importances (to be looked at later).\n",
    "\n",
    "```python\n",
    "config['IO']['feature_name'] = {\n",
    "    'name1': 'state',\n",
    "    'name2': 'state',\n",
    "    'name3': 'action',\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/config_model.yml') as conf:\n",
    "    config = yaml.full_load(conf)\n",
    "\n",
    "# TODO: Modify dictionary for as 'feature_name': 'action' or 'state'\n",
    "config['IO']['feature_name'] = {\n",
    "    'theta': 'state',\n",
    "    'alpha': 'state',\n",
    "    'theta_dot': 'state',\n",
    "    'alpha_dot': 'state',\n",
    "    'Vm': 'action',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 3`: Add the desired states to be predicted from the supervised learning simulator. Typically it is the next state after a timestep, $(\\underline{s}, \\underline{a}) \\rightarrow \\underline{s}'$. However, they may be additional features in the data that you may wish to actually predict. This is okay too, just make sure to have sufficient proxy information in the features to determine it.\n",
    "\n",
    "```python\n",
    "config['IO']['output_name'] = [\n",
    "    'name1',\n",
    "    'name2',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify list to consist of predicted states\n",
    "config['IO']['output_name'] = [\n",
    "    'theta',\n",
    "    'alpha',\n",
    "    'theta_dot',\n",
    "    'alpha_dot',\n",
    "]\n",
    "\n",
    "with open('config/config_model.yml', 'w') as conf:\n",
    "    yaml.dump(config, conf, sort_keys=False)\n",
    "\n",
    "feature_names = []\n",
    "for key, value in config['IO']['feature_name'].items():\n",
    "    feature_names.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test below checks if there are NaN (Not a Number) or SNA (Signals Not Available) based upon each csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad9b92bfca50abbda757f02f4caf9969",
     "grade": true,
     "grade_id": "cell-075b43e670fe45d3",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for x in check_nan:\n",
    "    assert(x == None or x == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 4`: Change the timelag or the number of iterations that span between the state transition, $(\\underline{s}, \\underline{a}) \\rightarrow \\underline{s}'$. Think of this as the number of rows in the csv that dictate the timestep between a \"steady state\" transition, where a change in an input to the system will be reflected after this many sample measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test below finds outliers for each dataset, plots the original states and actions with overlayed outliers marked. Outliers can occur due to noisy sensors, conditions that are abnormal, or if the Signal is Not Available (SNA) where it defaults to a really large or small number.\n",
    "\n",
    "- fits to data in single csv, check for if any data is outside 3 std\n",
    "- plots states and actions, overlayed with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "674c7db7e1c797d9a252d0e823084b4b",
     "grade": true,
     "grade_id": "cell-125c8ee9119954e1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Check for Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 5`: The notebook will prompt you to accept `Y/N` with the outliers. It is okay to have a few as long as they make sense to you and are not going to interrupt learning the normal conditions you expect the simulator to model (not abnormal).\n",
    "\n",
    "If you need to manipulate the data further, Enter `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35f0aa8ca6cb7b3093adc8c68aef98d9",
     "grade": true,
     "grade_id": "cell-ce12dc1e6c9d35fd",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "accept_outliers = input('Do you accept the outliers in the following dataset? Enter \"Yes\". If not, type \"No\" and filter or smooth data: ')\n",
    "\n",
    "assert(accept_outliers == 'Yes' or accept_outliers == 'yes' or accept_outliers == 'y' or accept_outliers == 'Y'), \"Manipulate data to smooth and filter to remove outliers before step 5, then re-run cells up until this point again\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After accepting the outliers, this notebook will concatenate the data and check for NaNs again due to any datasets missing features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7de33f17e8022cad0e8ddb12b2d56f6f",
     "grade": true,
     "grade_id": "cell-169a850c9712f865",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for df in df_list:\n",
    "  dfs = pd.concat([dfs, df[feature_names]], sort=False)\n",
    "\n",
    "print(dfs.head())\n",
    "\n",
    "check_nan = hasNaN(df.to_numpy())\n",
    "assert(check_nan == None or check_nan == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now qualified your datasets enough to export it to a single csv, named `approved_data.csv`. You have now finished the first section of `Data Relevance`. This does NOT say anything about data sparsity and distribution confidence, which are the next two sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.to_csv('approved_data.csv', mode='w', index=False)\n",
    "\n",
    "csv_to_pickle('approved_data.csv', timelag=timelag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below determines the `feature importances`, which are quantifying the inputs that are most valuable in explaining the target variable. The feature importances add up to one when summed, where the largest value is the most important feature. This is a useful trick in designing inputs/ouputs for supervised learning. We determine the feature importances for each of the predicted outputs, based on the features provided in cells above. They are then plotted where a legend designates the predicted value.\n",
    "\n",
    "> If you are NOT satisfied with your chosen model inputs, modify the inputs in the cell above and run through all the cells leading up to this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3541ed1860678eae820b207e07f43a3e",
     "grade": true,
     "grade_id": "cell-8094f7632eb6baee",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Feature Importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 6`: Enter `Y` if you accept your current model inputs and outputs, If not, Enter `N` and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a215f18e4e0d2ef053712860db6e872",
     "grade": true,
     "grade_id": "cell-c163b2f5a6ecfde6",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "accept_features = input('Do you accept the features based upon the feature importances shown? Enter \"Yes\" to continue. (Otherwise re-enter states and actions and run through cells again):  ')\n",
    "\n",
    "assert(accept_features == 'Yes' or accept_features == 'yes' or accept_features == 'y' or accept_features == 'Y'), \"Re-enter states and actions at step 2 and re-run through cells to visualize feature importances again.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION B: Sparsity\n",
    "\n",
    "- define Subject Matter Expert (SME) limits on feature ranges\n",
    "- plot histograms\n",
    "- compare SME limits with data limits using 2 std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "229ae571e3a7624eba984cb55c7fe7e3",
     "grade": true,
     "grade_id": "cell-0e275f4309a27519",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Template limits to copy/paste into next cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 7`: Modify template of min/max values for each of the features - this is where you define the Subject Matter Expert (SME) limits. Please define the range that would be reasonable to run the simulator in, despite what is captured in data. These are the limits that Reinforcement Learning will reasonably explore in to provide novel solutions.\n",
    "\n",
    "> Copy/Paste the above template below the `%%writefile config/sme_limits.yml` line and run the cell to write to the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config/sme_limits.yml\n",
    "theta:\n",
    "  min: -1.5708\n",
    "  max: 1.5708\n",
    "alpha:\n",
    "  min: -3.14159\n",
    "  max: 3.14159\n",
    "theta_dot:\n",
    "  min: -7.822916413465077\n",
    "  max: 7.610718493430506\n",
    "alpha_dot:\n",
    "  min: -12.841118663208904\n",
    "  max: 11.931522401818508\n",
    "Vm:\n",
    "  min: -3\n",
    "  max: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following test will plot the histograms for each of the features and check whether or not the data's mean $\\pm 2$ std is larger than the SME limits. \n",
    "\n",
    "> The tests assume your data has a gaussian distribution, i.e. bi-modal data can be problematic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab8a9291403b435e18df3e4716d9890a",
     "grade": true,
     "grade_id": "cell-22ed4f13332907f2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Histogram and report sparsity index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION C: Data Distribution Trust (Confidence on Interpolation)\n",
    "\n",
    "- evaluate region confidence with model upper bound with 2 std from mean\n",
    "- evaluate region confidence with model lower bound with 2 std from mean\n",
    "- evaluate region confidence with SME max\n",
    "- evaluate region confidence with SME min\n",
    "\n",
    "We use a Gaussian Mixture Model (GMM) to fit to the data to be able to cluster distributions with means and covariances. We can then sample the GMM with a random state-action pair and evaluate the regions to trust based compared to SME desired limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "036320a054fe88f7b5da8b87204e4120",
     "grade": true,
     "grade_id": "cell-d880180658ddb8e4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Create GMM using the same number of components as the number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da11b2f091944843c5fa3f1efa093106",
     "grade": true,
     "grade_id": "cell-7819433c69c3ca4c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with model upper bound with 2 std from mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "380e29c8482b16f203b882dffc4e32ba",
     "grade": true,
     "grade_id": "cell-b06bc007122199ad",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with model lower bound with 2 std from mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8af67ab8d405efc45941f13d46e61ba2",
     "grade": true,
     "grade_id": "cell-7d476fead68834b2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with SME max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28ab27881d3d646e87f6381a384f5910",
     "grade": true,
     "grade_id": "cell-8e3d3582046a56a1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with SME min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
