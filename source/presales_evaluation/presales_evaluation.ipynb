{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Driven Presales Evaluation\n",
    "\n",
    "Welcome to using the data driven jupyter notebook for evaluating if data from csv file(s) is sufficient for creating simulators using supervised learning methods. The approach of learning the state transitions, $(\\underline{s}, \\underline{a}) \\rightarrow \\underline{s}'$ from data is growing in popularity, however, not all of your data may have the correct distributions for the ranges needed for your Reinforcement Learning use case. This notebook is split up into three sections: \n",
    "\n",
    "- Data Relevance\n",
    "- Sparsity\n",
    "- Data Distribution Confidence\n",
    "\n",
    "This notebook uses `nbgrader` package to 'grade' your data quality, distributions, and feasibility of creating approximated simulations from your data. A score of 100 means you passed all the tests. The tests basically consists of assert conditions that individual notebook cells must run successfully, require user input of `Y/N` that you agree, or requiring inputs of Subject Matter Expert (SME) data ranges. The `nbgrader` package allows for certain snippets of code to be hidden from you to simplify the usage of this notebook. When code is hidden from you, you will know because the cell can NO longer be edited in jupyter notebook.\n",
    "\n",
    "> To pass tests, you may have to create a new cell and write code to filter/smooth/manipulate your data \n",
    "\n",
    "Successfully run all cells to assess whether or not a data driven simulator can be adequately created from your data. Once you have ran the cells without assertion errors, quickly double check your script passes the tests by click the `Validate` button in the jupyter notebook. If all tests are passed, then please export this as a PDF to share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "gitroot = os.popen('git rev-parse --show-toplevel').read()\n",
    "os.chdir(gitroot.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION A: Data Relevance\n",
    "\n",
    "- load csv file(s)\n",
    "- define potential inputs/outputs\n",
    "- check NaN\n",
    "- define cadence of state transitions\n",
    "- check outliers and plot each indiviual dataset\n",
    "- check NaN after concatenating\n",
    "- use feature importances to determine best features\n",
    "- re-define features (inputs)\n",
    "- save as single csv, named approved_data.csv\n",
    "\n",
    "`Step 1`: Add path to filenames as strings to the `filenames` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    './csv_data/example_data.csv',\n",
    "    #'./csv_data/faulty_data.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test below simply awards 10 points if the data can be successfuly loaded into the jupyter notebook, i.e. paths are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f646fd7fefe9fed1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for location in filenames:\n",
    "    read_datafile = pd.read_csv(location)\n",
    "    df_list.append(read_datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 2`: Add any potential feature names as states and actions to the dictionary in the following way. Here we want to be more broad than you think because this notebook will help you determine which features probably matter more using what's called feature importances (to be looked at later).\n",
    "\n",
    "```python\n",
    "config['IO']['feature_name'] = {\n",
    "    'name1': 'state',\n",
    "    'name2': 'state',\n",
    "    'name3': 'action',\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/config_model.yml') as conf:\n",
    "    config = yaml.full_load(conf)\n",
    "\n",
    "# TODO: Modify dictionary for as 'feature_name': 'action' or 'state'\n",
    "config['IO']['feature_name'] = {\n",
    "    'theta': 'state',\n",
    "    'alpha': 'state',\n",
    "    'theta_dot': 'state',\n",
    "    'alpha_dot': 'state',\n",
    "    'Vm': 'action',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 3`: Add the desired states to be predicted from the supervised learning simulator. Typically it is the next state after a timestep, $(\\underline{s}, \\underline{a}) \\rightarrow \\underline{s}'$. However, they may be additional features in the data that you may wish to actually predict. This is okay too, just make sure to have sufficient proxy information in the features to determine it.\n",
    "\n",
    "```python\n",
    "config['IO']['output_name'] = [\n",
    "    'name1',\n",
    "    'name2',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify list to consist of predicted states\n",
    "config['IO']['output_name'] = [\n",
    "    'theta',\n",
    "    'alpha',\n",
    "    'theta_dot',\n",
    "    'alpha_dot',\n",
    "]\n",
    "\n",
    "with open('config/config_model.yml', 'w') as conf:\n",
    "    yaml.dump(config, conf, sort_keys=False)\n",
    "\n",
    "feature_names = []\n",
    "for key, value in config['IO']['feature_name'].items():\n",
    "    feature_names.append(key)\n",
    "    \n",
    "output_names = config['IO']['output_name']\n",
    "\n",
    "req_keys = feature_names.copy()\n",
    "for name in output_names:\n",
    "    if name not in req_keys:\n",
    "        req_keys.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test below checks if there are NaN (Not a Number), strings, or objects based upon each csv. Building a data driven simulator using supervised learning methods must be provided numbers, it must be processed. You will be provided with which columns of which datasets have improper types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-075b43e670fe45d3",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Check for strings, nans, other datatypes than numbers\n",
    "\n",
    "def hasNaN(x_set):\n",
    "    nan_catcher = np.zeros((1, x_set.shape[1]))\n",
    "    catcher = []\n",
    "\n",
    "    if isinstance(x_set, pd.DataFrame):\n",
    "        i = 0\n",
    "        for column_name, column_data in x_set.iteritems():\n",
    "            catcher_col = []\n",
    "            if column_data.dtype == 'object':\n",
    "                for row in column_data.values:\n",
    "                    if pd.isnull(row):\n",
    "                        nan_catcher[0, i] += 1\n",
    "                    else:\n",
    "                        try:\n",
    "                            row = float(row)\n",
    "                        except:\n",
    "                            catcher_col.append(row)\n",
    "            else:\n",
    "                pass\n",
    "            catcher.append(catcher_col)\n",
    "            i += 1\n",
    "\n",
    "    elif isinstance(x_set, np.ndarray):\n",
    "        for i in range (0, x_set.shape[1]):\n",
    "            catcher_col = []\n",
    "            for j in range(x_set.shape[0]):\n",
    "                if type(x_set[j, i]) == str or type(x_set[j, i]) == list or type(x_set[j, i]) == dict or type(x_set[j, i]) == tuple:\n",
    "                    try:\n",
    "                        row = float(x_set[j, i])\n",
    "                    except:\n",
    "                        catcher_col.append(x_set[j, i])\n",
    "                elif pd.isnull(x_set[j, i]):\n",
    "                    nan_catcher[0, i] += 1\n",
    "                else:\n",
    "                    pass\n",
    "            catcher.append(catcher_col)\n",
    "    else:\n",
    "        print('Please input pandas dataframe or numpy array')\n",
    "        exit()\n",
    "    return nan_catcher, catcher\n",
    "\n",
    "check_nan = []\n",
    "check_ = []\n",
    "for i, df in enumerate(df_list):\n",
    "    print('data set: {}'.format(filenames[i]))\n",
    "    nan_count, _ = hasNaN(df[req_keys])\n",
    "    check_nan.append(nan_count)\n",
    "    check_.append(_)\n",
    "    for i in range(0, df[req_keys].shape[1]):\n",
    "        print('Detected {} NaN and the following issues in column, {}:  {}'.format(int(nan_count[0, i]), req_keys[i], set(_[i])))\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for dataset in check_nan:\n",
    "    for col in dataset:\n",
    "        assert(col.any() < 1), \"NaNs were detected, remove.\"\n",
    "for dataset in check_:\n",
    "    for col in dataset:\n",
    "        assert(len(col) == 0), \"Remove undesired data samples tagged above.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 4`: Change the timelag or the number of iterations that span between the state transition, $(\\underline{s}, \\underline{a}) \\rightarrow \\underline{s}'$. Think of this as the number of rows in the csv that dictate the timestep between a \"steady state\" transition, where a change in an input to the system will be reflected after this many sample measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test below finds outliers for each dataset, plots the original states and actions with overlayed outliers marked. Outliers can occur due to noisy sensors, conditions that are abnormal, or if the Signal is Not Available (SNA) where it defaults to a really large or small number.\n",
    "\n",
    "- fits to data in single csv, check for if any data is outside 2 std\n",
    "- plots states and actions, overlayed with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-125c8ee9119954e1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Check for Outliers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from env_data_modeler import env_gb_modeler\n",
    "\n",
    "def plotOutliers(y_set, y_predict_all, outlier_data, config):\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    numSubPlots = y_set.shape[1]\n",
    "\n",
    "    outlierData = outlier_data['y' + str(0)]\n",
    "    dataLabel = []\n",
    "    for key, value in config['IO']['feature_name'].items():\n",
    "        if value == 'state':\n",
    "            dataLabel.append(key)\n",
    "\n",
    "    ax1 = plt.subplot(1, 1, 0+1)\n",
    "    plt.plot(y_set[:,0], label=dataLabel[0], linewidth=1, color = 'blue' )\n",
    "    plt.plot(y_predict_all[0], label=dataLabel[0], linewidth=1, color = 'black' )\n",
    "    plt.scatter(outlierData,y_set[outlierData,0], label='outlier', linewidth=1, marker = '*', color = 'red', s = 50)\n",
    "    plt.xticks(rotation='horizontal')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    for i in range(1,numSubPlots, 2):\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        outlierData = outlier_data['y' + str(i)]\n",
    "        for j in range(2):\n",
    "            try:\n",
    "                ax2 = plt.subplot(2, 1, j+1, sharex=ax1)\n",
    "                plt.plot(y_set[:,i+j], label=dataLabel[i+j], linewidth=1, color = 'blue' )\n",
    "                plt.plot(y_predict_all[i+j], label=dataLabel[i+j], linewidth=1, color = 'black' )\n",
    "                plt.scatter(outlierData,y_set[outlierData,i+j], label='outlier', linewidth=1, marker = '*', color = 'red', s = 50)\n",
    "                plt.xticks(rotation='horizontal')\n",
    "                plt.legend(loc='upper right')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # plt.show()\n",
    "    \n",
    "def findOutliersAll(x_set,y_set, thrhld=2):\n",
    "    ## Computing Feature importance using gradient boosting\n",
    "    print('computing Outliers ....')\n",
    "    outlier_data = {}\n",
    "    y_predict_all = []\n",
    "    for i in range (0, y_set.shape[1]):\n",
    "        gb_estimator=GradientBoostingRegressor(n_iter_no_change=50, validation_fraction=.2)\n",
    "        gb_model= gb_estimator.fit(x_set,y_set[:,i])\n",
    "        y_predict = gb_estimator.predict(x_set)\n",
    "        outlier_data['y' + str(i)] = findOutlier(y_set[:,i], y_predict, thrhld=thrhld)\n",
    "        y_predict_all.append(y_predict)\n",
    "        print('y', str(i), ': ', outlier_data['y' + str(i)])\n",
    "    return outlier_data, y_predict_all\n",
    "\n",
    "def findOutlier(y, y_predict, thrhld=2):\n",
    "    y_std = y.std(axis = 0)\n",
    "    outL = np.where(np.abs(y-y_predict) > thrhld*y_std) # tuple\n",
    "    return outL[0]\n",
    "\n",
    "def plotInputs(x_set, y_set, config):\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    numSubPlots = x_set.shape[1] - y_set.shape[1]  ## Num of inputs\n",
    "    \n",
    "    dataLabel = []\n",
    "    for key, value in config['IO']['feature_name'].items():\n",
    "        if value == 'action':\n",
    "            dataLabel.append(key)\n",
    "    ax1 = plt.subplot(numSubPlots, 1, 1)\n",
    "    plt.plot(x_set[:,y_set.shape[1]+0], label=dataLabel[0], linewidth=1, color = 'blue' )\n",
    "    plt.xticks(rotation='horizontal')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    for i in range(1,numSubPlots):\n",
    "        ax2 = plt.subplot(numSubPlots, 1, i+1, sharex=ax1)\n",
    "        plt.plot(x_set[:,y_set.shape[1]+i], label=dataLabel[i], linewidth=1, color = 'blue' )\n",
    "        plt.xticks(rotation='horizontal')\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "    # plt.show()\n",
    "    return fig\n",
    "\n",
    "def maxMinMeanStd(x, varName = 'x'):\n",
    "    return x.min(axis = 0), x.max(axis = 0), x.mean(axis = 0), x.std(axis = 0)\n",
    "\n",
    "def df_to_set(logdf, timelag=1):\n",
    "    state_key_list = []\n",
    "    action_key_list = []\n",
    "    for key, value in config['IO']['feature_name'].items():\n",
    "        if value == 'state':\n",
    "            state_key_list.append(key)\n",
    "        elif value == 'action':\n",
    "            action_key_list.append(key)\n",
    "        else:\n",
    "            print('Please fix config_model.yml to specify either state or action')\n",
    "            exit()\n",
    "    output_key_list = config['IO']['output_name']\n",
    "\n",
    "    outputs = logdf[output_key_list]\n",
    "    states = logdf[state_key_list]\n",
    "    actions = logdf[action_key_list]\n",
    "\n",
    "    states_t = states.iloc[0:-timelag]\n",
    "    states_tplus1 = outputs.iloc[timelag:]\n",
    "    len(states_t)\n",
    "    len(states_tplus1)\n",
    "    actions_t = actions.iloc[0:-timelag]\n",
    "    frames = [states_t, actions_t]\n",
    "    x_set_df = pd.concat(frames, axis=1)\n",
    "    y_set_df = states_tplus1\n",
    "    \n",
    "    x_set = x_set_df.to_numpy()\n",
    "    y_set = y_set_df.to_numpy()\n",
    "    \n",
    "    return x_set, y_set\n",
    "\n",
    "for i, df in enumerate(df_list):\n",
    "    print('data set: {}'.format(filenames[i]))\n",
    "    x_set, y_set = df_to_set(df[req_keys])\n",
    "    outlier_data, y_predict_all = findOutliersAll(x_set, y_set, thrhld=2)\n",
    "    fig = plotOutliers(y_set, y_predict_all, outlier_data, config)\n",
    "    plotInputs(x_set, y_set, config)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 5`: The notebook will prompt you to accept `Y/N` with the outliers. It is okay to have a few as long as they make sense to you and are not going to interrupt learning the normal conditions you expect the simulator to model (not abnormal).\n",
    "\n",
    "If you need to manipulate the data further, Enter `N`. Manipulate data to smooth and filter to remove outliers before step 5, then re-run cells up until this point again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_outliers = input('Do you accept the outliers in the following dataset? Enter \"Yes\". If not, type \"No\" and filter or smooth data: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After accepting the outliers, this notebook will concatenate the data and check for NaNs again due to any datasets missing features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-169a850c9712f865",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for df in df_list:\n",
    "  dfs = pd.concat([dfs, df[req_keys]], sort=False)\n",
    "\n",
    "print(dfs.head())\n",
    "\n",
    "nan_count, _ = hasNaN(df[req_keys])\n",
    "for i in range(0, df[req_keys].shape[1]):\n",
    "    print('Detected {} NaN and the following issues in column, {}:  {}'.format(int(nan_count[0, i]), req_keys[i],  set(_[i])))\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for col in nan_count:\n",
    "    assert(col.any() < 1), \"NaNs were detected, remove.\"\n",
    "for col in _:\n",
    "    assert(len(col) == 0), \"Remove undesired data samples tagged above.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now qualified your datasets enough to export it to a single csv, named `processed_data.csv`. You have now finished the first section of `Data Relevance`. This does NOT say anything about data sparsity and distribution confidence, which are the next two sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodeler import csv_to_pickle\n",
    "\n",
    "dfs.to_csv('processed_data.csv', mode='w', index=False)\n",
    "\n",
    "csv_to_pickle('processed_data.csv', timelag=timelag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below determines the `feature importances`, which are quantifying the inputs that are most valuable in explaining the target variable. The feature importances add up to one when summed, where the largest value is the most important feature. This is a useful trick in designing inputs/ouputs for supervised learning. We determine the feature importances for each of the predicted outputs, based on the features provided in cells above. They are then plotted where a legend designates the predicted value.\n",
    "\n",
    "> If you are NOT satisfied with your chosen model inputs, modify the inputs in the cell above and run through all the cells leading up to this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8094f7632eb6baee",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Feature Importances\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from env_data_modeler import env_gb_modeler\n",
    "from datamodeler import read_env_data\n",
    "\n",
    "def feature_plots(feature_data, state_space_dim, action_space_dim, config, total_width=0.5):\n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']  \n",
    "    n_bars = len(feature_data)\n",
    "    bar_width = total_width / n_bars # width of single bar\n",
    "    bars = []\n",
    "    for i, (name, values) in enumerate(feature_data.items()):\n",
    "        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2\n",
    "        for x, y in enumerate(values):\n",
    "            bar = ax.bar(x + x_offset, y, width=bar_width , color=colors[i % len(colors)])\n",
    "        bars.append(bar[0])\n",
    "\n",
    "\n",
    "    ax.legend(bars, feature_data.keys())\n",
    "    plt.xlabel('Feature Number', fontsize=18)\n",
    "    plt.ylabel('Feature Importance', fontsize=18)\n",
    "\n",
    "    plt.xticks(ticks=range(state_space_dim+action_space_dim), labels=config['IO']['feature_name'].keys())\n",
    "    ax.tick_params(labelrotation=90)\n",
    "    plt.show()\n",
    "\n",
    "state_space_dim = 0\n",
    "action_space_dim = 0\n",
    "for key, value in config['IO']['feature_name'].items():\n",
    "    if value == 'state':\n",
    "        state_space_dim += 1\n",
    "    elif value == 'action':\n",
    "        action_space_dim += 1\n",
    "    else:\n",
    "        print('Please fix config_model.yml to specify either state or action')\n",
    "        \n",
    "x_set, y_set = read_env_data()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_set, y_set, test_size=0.33, random_state=42)\n",
    "\n",
    "## Computing Feature importance using gradient boosting\n",
    "print('computing Feature Importance ....')\n",
    "feature_importance_data = {}\n",
    "for i in range (0, y_set.shape[1]):\n",
    "    gb_estimator=env_gb_modeler(state_space_dim, action_space_dim)\n",
    "    gb_estimator.create_gb_model()\n",
    "    gb_model= gb_estimator.train_gb_model(x_train,y_train[:,i])\n",
    "    feature_importance_data[feature_names[i]+\"'\"] = gb_model.feature_importances_\n",
    "    print('feature importance for', feature_names[i]+\"'\", ' :', feature_importance_data[feature_names[i]+\"'\"])\n",
    "feature_plots(feature_importance_data, state_space_dim, action_space_dim, config, total_width=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 6`: Enter `Y` if you accept your current model inputs and outputs, If not, Enter `N` and Re-enter states and actions at step 2 and re-run through cells to visualize feature importances again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_features = input('Do you accept the features based upon the feature importances shown? Enter \"Yes\" to continue. (Otherwise re-enter states and actions and run through cells again):  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION B: Sparsity\n",
    "\n",
    "- define Subject Matter Expert (SME) limits on feature ranges\n",
    "- plot histograms\n",
    "- compare SME limits with data limits using 2 std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e275f4309a27519",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Template limits to copy/paste into next cell\n",
    "\n",
    "sme_limits = {}\n",
    "for f in feature_names:\n",
    "    sme_limits.update({\n",
    "        f: {\n",
    "            'min': -1,\n",
    "            'max': 1,\n",
    "        }\n",
    "    })\n",
    "\n",
    "with open('config/sme_limits.yml', 'w') as conf:\n",
    "    yaml.dump(sme_limits, conf, sort_keys=False)\n",
    "    \n",
    "with open('config/sme_limits.yml', 'r') as sme:\n",
    "    print(sme.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Step 7`: Modify template of min/max values for each of the features - this is where you define the Subject Matter Expert (SME) limits. Please define the range that would be reasonable to run the simulator in, despite what is captured in data. These are the limits that Reinforcement Learning will reasonably explore in to provide novel solutions.\n",
    "\n",
    "> Copy/Paste the above template below the `%%writefile config/sme_limits.yml` line and run the cell to write to the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config/sme_limits.yml\n",
    "theta:\n",
    "  min: -1.5708\n",
    "  max: 1.5708\n",
    "alpha:\n",
    "  min: -3.14159\n",
    "  max: 3.14159\n",
    "theta_dot:\n",
    "  min: -7.822916413465077\n",
    "  max: 7.610718493430506\n",
    "alpha_dot:\n",
    "  min: -12.841118663208904\n",
    "  max: 11.931522401818508\n",
    "Vm:\n",
    "  min: -3\n",
    "  max: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following test will plot the histograms for each of the features and check whether or not the data's mean $\\pm 2$ std is larger than the SME limits. \n",
    "\n",
    "> The tests assume your data has a gaussian distribution, i.e. bi-modal data can be problematic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-22ed4f13332907f2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Histogram and report sparsity index\n",
    "\n",
    "with open('config/sme_limits.yml', 'r') as sme:\n",
    "    sme_limits = yaml.full_load(sme)\n",
    "\n",
    "with open('config/model_limits.yml') as conf:\n",
    "    model_limits = yaml.full_load(conf)\n",
    "\n",
    "for j in range(0, len(feature_names), 3):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20,15), sharey=True)\n",
    "    for i, f in enumerate(feature_names[j:j+3]):\n",
    "        (n, bins, patches) = axs[i].hist(x_set[:, i], bins=100)\n",
    "        plt.setp(axs[i], xlabel=feature_names[i])\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for key in config['IO']['feature_name'].keys():\n",
    "    model_mean = model_limits[key]['mean']\n",
    "    model_std = model_limits[key]['std']\n",
    "    assert(model_mean + 2 * model_std >= sme_limits[key]['max'] and model_mean - 2 * model_std <= sme_limits[key]['min'])\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION C: Data Distribution Trust (Confidence on Interpolation)\n",
    "\n",
    "- evaluate region confidence with model upper bound with 2 std from mean\n",
    "- evaluate region confidence with model lower bound with 2 std from mean\n",
    "- evaluate region confidence with SME max\n",
    "- evaluate region confidence with SME min\n",
    "\n",
    "We use a Gaussian Mixture Model (GMM) to fit to the data to be able to cluster distributions with means and covariances. We can then test the GMM with a random state-action pair and evaluate the regions to trust based compared to SME desired limits. \n",
    "\n",
    "> Note: Section C needs improvement, GMMs are probably not helpful here as much as KL-divergence or MMD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d880180658ddb8e4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Create GMM using the same number of components as the number of features\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.99, random_state=0)\n",
    "pca_data = pca.fit_transform(dfs)\n",
    "initial_n_components = pca_data.shape[1]\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "n_components = len(feature_names)\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=n_components,\n",
    "    covariance_type='full',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gmm.fit(dfs)\n",
    "\n",
    "#print(gmm.means_)\n",
    "#print(gmm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7819433c69c3ca4c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with model upper bound with 2 std from mean\n",
    "\n",
    "\n",
    "sample = []\n",
    "for feature_name in feature_names:\n",
    "    sample.append(model_limits[feature_name]['mean']+2*model_limits[feature_name]['std'])\n",
    "sample = np.array(sample).reshape(1, len(feature_names))\n",
    "print('sample within model max: {}'.format(sample))\n",
    "prob_result = gmm.predict_proba(sample)\n",
    "print('probability', np.ravel(prob_result))\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "most_likely = max(np.ravel(prob_result))\n",
    "assert(most_likely > threshold and most_likely != 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b06bc007122199ad",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with model lower bound with 2 std from mean\n",
    "\n",
    "sample = []\n",
    "for feature_name in feature_names:\n",
    "    sample.append(model_limits[feature_name]['mean']-2*model_limits[feature_name]['std'])\n",
    "sample = np.array(sample).reshape(1, len(feature_names))\n",
    "print('sample within model min: {}'.format(sample))    \n",
    "prob_result = gmm.predict_proba(sample)\n",
    "print('probability', np.ravel(prob_result))\n",
    "\n",
    "most_likely = max(np.ravel(prob_result))\n",
    "assert(most_likely > threshold and most_likely != 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7d476fead68834b2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with SME max\n",
    "\n",
    "sample = []\n",
    "for feature_name in feature_names:\n",
    "    sample.append(sme_limits[feature_name]['max'])\n",
    "sample = np.array(sample).reshape(1, len(feature_names))\n",
    "print('sample at SME max: {}'.format(sample))\n",
    "prob_result = gmm.predict_proba(sample)\n",
    "print('probability', np.ravel(prob_result))\n",
    "\n",
    "most_likely = max(np.ravel(prob_result))\n",
    "assert(most_likely > threshold and most_likely != 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8e3d3582046a56a1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate region confidence with SME min\n",
    "\n",
    "sample = []\n",
    "for feature_name in feature_names:\n",
    "    sample.append(sme_limits[feature_name]['min'])\n",
    "sample = np.array(sample).reshape(1, len(feature_names))\n",
    "print('sample at SME min: {}'.format(sample))\n",
    "prob_result = gmm.predict_proba(sample)\n",
    "print('probability', np.ravel(prob_result))\n",
    "\n",
    "most_likely = max(np.ravel(prob_result))\n",
    "assert(most_likely > threshold and most_likely != 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
